{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BD_LSTM_Name_Entity_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIxKFX6GyHdyamPS4QnSij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Finalmatt/pandorabox/blob/master/BD_GRU_Name_Entity_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DMpd61YddS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "\n",
        "\n",
        "def download_github_code(path):\n",
        "    filename = path.rsplit(\"/\")[-1]\n",
        "    os.system(\"wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/{} -O {}\".format(path, filename))\n",
        "\n",
        "\n",
        "def setup_common():\n",
        "    download_github_code(\"common/requirements_colab.txt\")\n",
        "    download_github_code(\"common/download_utils.py\")\n",
        "    download_github_code(\"common/tqdm_utils.py\")\n",
        "    download_github_code(\"common/__init__.py\")\n",
        "    os.system(\"mkdir common\")\n",
        "    os.system(\"mv download_utils.py tqdm_utils.py __init__.py common/\")\n",
        "    os.system(\"mv requirements_colab.txt common/\")\n",
        "\n",
        "    os.system(\"pip install -r common/requirements_colab.txt --force-reinstall\")\n",
        "\n",
        "\n",
        "def setup_starspace():\n",
        "    if not os.path.exists(\"/usr/local/bin/starspace\"):\n",
        "        os.system(\"wget https://dl.bintray.com/boostorg/release/1.63.0/source/boost_1_63_0.zip\")\n",
        "        os.system(\"unzip boost_1_63_0.zip && mv boost_1_63_0 /usr/local/bin\")\n",
        "        os.system(\"git clone https://github.com/facebookresearch/Starspace.git\")\n",
        "        os.system(\"cd Starspace && make && cp -Rf starspace /usr/local/bin\")\n",
        "\n",
        "\n",
        "def setup_week1():\n",
        "    setup_common()\n",
        "    download_github_code(\"week1/grader.py\")\n",
        "    download_github_code(\"week1/metrics.py\")\n",
        "\n",
        "\n",
        "def setup_week2():\n",
        "    setup_common()\n",
        "    download_github_code(\"week2/evaluation.py\")\n",
        "\n",
        "\n",
        "def setup_week3():\n",
        "    setup_common()\n",
        "    download_github_code(\"week3/grader.py\")\n",
        "    download_github_code(\"week3/util.py\")\n",
        "    setup_starspace()\n",
        "\n",
        "\n",
        "def setup_week4():\n",
        "    setup_common()\n",
        "\n",
        "\n",
        "def setup_project():\n",
        "    setup_common()\n",
        "    download_github_code(\"project/dialogue_manager.py\")\n",
        "    download_github_code(\"project/main_bot.py\")\n",
        "    download_github_code(\"project/utils.py\")\n",
        "    setup_starspace()\n",
        "\n",
        "\n",
        "def setup_honor():\n",
        "    setup_common()\n",
        "    download_github_code(\"honor/datasets.py\")\n",
        "    download_github_code(\"honor/example.py\")\n",
        "    download_github_code(\"honor/download_cornell.sh\")\n",
        "    download_github_code(\"honor/download_opensubs.sh\")\n",
        "\n",
        "setup_week2()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqZqldUqdhnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from common.download_utils import download_week2_resources\n",
        "\n",
        "download_week2_resources()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38JMsqY2eVEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.15.\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTcvfRjdh83o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            # Replace all urls with <URL> token\n",
        "            if token.startswith('http://'):\n",
        "              token = '<URL>' \n",
        "            elif token.startswith('https://'):\n",
        "              token = '<URL>' \n",
        "                       \n",
        "            # Replace all users with <USR> token\n",
        "            if token.startswith('@'):\n",
        "              token = '<USR>'\n",
        "            \n",
        "            \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCr1EoO2iK1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokens, train_tags = read_data('data/train.txt')\n",
        "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
        "test_tokens, test_tags = read_data('data/test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwuYiKit2ZYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(3):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH2RrwBA2pgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDXLGA3A3dNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    # Create mappings from tokens (or tags) to indices and vice versa.\n",
        "    # At first, add special tokens (or tags) to the dictionaries.\n",
        "    # The first special token must have index 0.\n",
        "    \n",
        "    # Mapping tok2idx should contain each token or tag only once. \n",
        "    # To do so, you should:\n",
        "    # 1. extract unique tokens/tags from the tokens_or_tags variable, which is not\n",
        "    #    occur in special_tokens (because they could have non-empty intersection)\n",
        "    # 2. index them (for example, you can add them into the list idx2tok\n",
        "    # 3. for each token/tag save the index into tok2idx).\n",
        "    for token in special_tokens:\n",
        "      idx2tok.append(token)\n",
        "    for line in tokens_or_tags:\n",
        "      for token in line:\n",
        "        if token not in special_tokens:\n",
        "          if token not in idx2tok:\n",
        "            idx2tok.append(token)  \n",
        "\n",
        "    for n in range(len(idx2tok)):\n",
        "      tok2idx[idx2tok[n]] = n\n",
        "\n",
        "    return tok2idx, idx2tok\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXfssLyj6d2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6dFDd9o9P6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM_h0lruG9LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batches_generator(batch_size, tokens, tags,\n",
        "                      shuffle=True, allow_smaller_last_batch=True):\n",
        "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
        "    \n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        "\n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        "\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "            \n",
        "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkBbKG5xHQTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z-O3AguHWNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTMModel( ):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THkskOQHHYxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def declare_placeholders(self):\n",
        "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
        "\n",
        "    # Placeholders for input and ground truth output.\n",
        "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
        "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags') \n",
        "  \n",
        "    # Placeholder for lengths of the sequences.\n",
        "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
        "    \n",
        "    # Placeholder for a dropout keep probability. If we don't feed\n",
        "    # a value for this placeholder, it will be equal to 1.0.\n",
        "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
        "    \n",
        "    # Placeholder for a learning rate (tf.float32).\n",
        "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_7WrWNAgMFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8adZ1DWhgjrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
        "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
        "    \n",
        "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
        "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
        "    embedding_matrix_variable = tf.Variable(initial_value = initial_embedding_matrix, dtype = tf.float32, name = 'embeddings_matrix')\n",
        "    \n",
        "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
        "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
        "    forward_cell =  tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(num_units = n_hidden_rnn),input_keep_prob = self.dropout_ph,output_keep_prob=self.dropout_ph,state_keep_prob=self.dropout_ph) \n",
        "    backward_cell =  tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(num_units = n_hidden_rnn),input_keep_prob = self.dropout_ph,output_keep_prob=self.dropout_ph,state_keep_prob=self.dropout_ph) \n",
        "\n",
        "\n",
        "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
        "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
        "    embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable ,self.input_batch)\n",
        "    \n",
        "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
        "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
        "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
        "    (rnn_output_fw, rnn_output_bw), _ =  tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, embeddings, dtype=tf.float32,\n",
        "    sequence_length=self.lengths)\n",
        "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
        "\n",
        "    # Dense layer on top.\n",
        "    # Shape: [batch_size, sequence_len, n_tags].   \n",
        "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6a81rHfBS1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.__build_layers = classmethod(build_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pYMoDGkCEdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_predictions(self):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
        "    \n",
        "    # Create softmax (tf.nn.softmax) function\n",
        "    softmax_output = tf.nn.softmax(self.logits)\n",
        "    \n",
        "    # Use argmax (tf.argmax) to get the most probable tags\n",
        "    # Don't forget to set axis=-1\n",
        "    # otherwise argmax will be calculated in a wrong way\n",
        "    self.predictions = tf.argmax(softmax_output,axis = -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaFntyYkCkhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL0eXWM8Cye5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(self, n_tags, PAD_index):\n",
        "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
        "    \n",
        "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits_v2)\n",
        "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
        "    loss_tensor =  tf.nn.softmax_cross_entropy_with_logits_v2(ground_truth_tags_one_hot,self.logits)\n",
        "    \n",
        "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
        "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
        "    # Be careful that the argument of tf.reduce_mean should be\n",
        "    # multiplication of mask and loss_tensor.\n",
        "    self.loss =  tf.reduce_mean(mask*loss_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCXS3eyzD6_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "BiLSTMModel.__compute_loss = classmethod(compute_loss)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bFNxSupFz82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def perform_optimization(self):\n",
        "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
        "    \n",
        "    # Create an optimizer (tf.train.AdamOptimizer)\n",
        "    self.optimizer =  tf.train.AdamOptimizer(self.learning_rate_ph)\n",
        "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "     \n",
        "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
        "    # Pay attention that you need to apply this operation only for gradients \n",
        "    # because self.grads_and_vars also contains variables.\n",
        "    # list comprehension might be useful in this case.\n",
        "    clip_norm = tf.cast(1.0, tf.float32)\n",
        "    self.grads_and_vars =  [(tf.clip_by_norm(g,clip_norm), v) for g,v in self.grads_and_vars]\n",
        "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLdUwbI6OPAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl4F6WrNOfn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
        "    self.__declare_placeholders()\n",
        "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
        "    self.__compute_predictions()\n",
        "    self.__compute_loss(n_tags, PAD_index)\n",
        "    self.__perform_optimization()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9-zhrl-OkVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.__init__ = classmethod(init_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu5F7swVOzSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.ground_truth_tags: y_batch,\n",
        "                 self.learning_rate_ph: learning_rate,\n",
        "                 self.dropout_ph: dropout_keep_probability,\n",
        "                 self.lengths: lengths}\n",
        "    \n",
        "    session.run(self.train_op, feed_dict=feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_9NbA_xPByg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_njEyOF1PG_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def predict_for_batch(self, session, x_batch, lengths):\n",
        "    predictions = session.run(self.predictions, feed_dict={\n",
        "        self.input_batch:x_batch,\n",
        "        self.lengths:lengths\n",
        "    })  \n",
        "\n",
        "    return predictions\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky2_Ku7hRHBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiZO9_BYRPUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluation import precision_recall_f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bXWTIwzRQKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_tags(model, session, token_idxs_batch, lengths):\n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        "    \n",
        "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
        "    \n",
        "    tags_batch, tokens_batch = [], []\n",
        "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
        "        tags, tokens = [], []\n",
        "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
        "            #print(tag_idxs)\n",
        "            tags.append(idx2tag[tag_idx])\n",
        "            tokens.append(idx2token[token_idx])\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "    return tags_batch, tokens_batch\n",
        "    \n",
        "    \n",
        "def eval_conll(model, session, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
        "        if len(x_batch[0]) != len(tags_batch[0]):\n",
        "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
        "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
        "        predicted_tags = []\n",
        "        ground_truth_tags = []\n",
        "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
        "            if token != '<PAD>':\n",
        "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
        "                predicted_tags.append(pred_tag)\n",
        "\n",
        "        # We extend every prediction and ground truth sequence with 'O' tag\n",
        "        # to indicate a possible end of entity.\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(predicted_tags + ['O'])\n",
        "        \n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Peeu0iytRv6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "model = BiLSTMModel( vocabulary_size= len(idx2token), n_tags=len(idx2tag),embedding_dim= 300, n_hidden_rnn= 300, PAD_index = token2idx['<PAD>'])\n",
        "\n",
        "batch_size = 32\n",
        "n_epochs = 5\n",
        "learning_rate = 0.005\n",
        "learning_rate_decay = np.sqrt(2)\n",
        "dropout_keep_probability = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-vWFrDyV3av",
        "colab_type": "code",
        "outputId": "17813490-00a6-4ed2-ca49-ffc884d1850f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Start training... \\n')\n",
        "for epoch in range(n_epochs):\n",
        "    # For each epoch evaluate the model on train and validation data\n",
        "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
        "    print('Train data evaluation:')\n",
        "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
        "    print('Validation data evaluation:')\n",
        "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
        "    \n",
        "    # Train the model\n",
        "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
        "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
        "        \n",
        "    # Decaying the learning rate\n",
        "    learning_rate = learning_rate / learning_rate_decay\n",
        "    \n",
        "print('...training finished.')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training... \n",
            "\n",
            "-------------------- Epoch 1 of 5 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 78756 phrases; correct: 162.\n",
            "\n",
            "precision:  0.21%; recall:  3.61%; F1:  0.39\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 9558 phrases; correct: 24.\n",
            "\n",
            "precision:  0.25%; recall:  4.47%; F1:  0.48\n",
            "\n",
            "-------------------- Epoch 2 of 5 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 3024 phrases; correct: 419.\n",
            "\n",
            "precision:  13.86%; recall:  9.33%; F1:  11.15\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 223 phrases; correct: 38.\n",
            "\n",
            "precision:  17.04%; recall:  7.08%; F1:  10.00\n",
            "\n",
            "-------------------- Epoch 3 of 5 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4953 phrases; correct: 1750.\n",
            "\n",
            "precision:  35.33%; recall:  38.98%; F1:  37.07\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 357 phrases; correct: 131.\n",
            "\n",
            "precision:  36.69%; recall:  24.39%; F1:  29.31\n",
            "\n",
            "-------------------- Epoch 4 of 5 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4819 phrases; correct: 2870.\n",
            "\n",
            "precision:  59.56%; recall:  63.93%; F1:  61.67\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 343 phrases; correct: 169.\n",
            "\n",
            "precision:  49.27%; recall:  31.47%; F1:  38.41\n",
            "\n",
            "-------------------- Epoch 5 of 5 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4676 phrases; correct: 3491.\n",
            "\n",
            "precision:  74.66%; recall:  77.77%; F1:  76.18\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 347 phrases; correct: 183.\n",
            "\n",
            "precision:  52.74%; recall:  34.08%; F1:  41.40\n",
            "\n",
            "...training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE9eHBFNxuEZ",
        "colab_type": "code",
        "outputId": "eebe0371-1877-432d-ebbf-e79815f6a284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
        "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
        "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False)\n",
        "\n",
        "\n",
        "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
        "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Train set quality: --------------------\n",
            "processed 105778 tokens with 4489 phrases; found: 4674 phrases; correct: 3753.\n",
            "\n",
            "precision:  80.30%; recall:  83.60%; F1:  81.92\n",
            "\n",
            "\t     company: precision:   87.01%; recall:   89.58%; F1:   88.28; predicted:   662\n",
            "\n",
            "\t    facility: precision:   78.17%; recall:   84.39%; F1:   81.16; predicted:   339\n",
            "\n",
            "\t     geo-loc: precision:   87.94%; recall:   95.18%; F1:   91.42; predicted:  1078\n",
            "\n",
            "\t       movie: precision:   45.00%; recall:   26.47%; F1:   33.33; predicted:    40\n",
            "\n",
            "\t musicartist: precision:   65.52%; recall:   57.33%; F1:   61.15; predicted:   203\n",
            "\n",
            "\t       other: precision:   72.00%; recall:   76.75%; F1:   74.30; predicted:   807\n",
            "\n",
            "\t      person: precision:   86.76%; recall:   92.44%; F1:   89.51; predicted:   944\n",
            "\n",
            "\t     product: precision:   67.66%; recall:   78.30%; F1:   72.59; predicted:   368\n",
            "\n",
            "\t  sportsteam: precision:   72.17%; recall:   70.51%; F1:   71.33; predicted:   212\n",
            "\n",
            "\t      tvshow: precision:   52.38%; recall:   18.97%; F1:   27.85; predicted:    21\n",
            "\n",
            "-------------------- Validation set quality: --------------------\n",
            "processed 12836 tokens with 537 phrases; found: 345 phrases; correct: 185.\n",
            "\n",
            "precision:  53.62%; recall:  34.45%; F1:  41.95\n",
            "\n",
            "\t     company: precision:   64.20%; recall:   50.00%; F1:   56.22; predicted:    81\n",
            "\n",
            "\t    facility: precision:   48.00%; recall:   35.29%; F1:   40.68; predicted:    25\n",
            "\n",
            "\t     geo-loc: precision:   67.82%; recall:   52.21%; F1:   59.00; predicted:    87\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
            "\n",
            "\t musicartist: precision:   25.00%; recall:    3.57%; F1:    6.25; predicted:     4\n",
            "\n",
            "\t       other: precision:   43.40%; recall:   28.40%; F1:   34.33; predicted:    53\n",
            "\n",
            "\t      person: precision:   44.26%; recall:   24.11%; F1:   31.21; predicted:    61\n",
            "\n",
            "\t     product: precision:   25.00%; recall:   14.71%; F1:   18.52; predicted:    20\n",
            "\n",
            "\t  sportsteam: precision:   46.15%; recall:   30.00%; F1:   36.36; predicted:    13\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "-------------------- Test set quality: --------------------\n",
            "processed 13258 tokens with 604 phrases; found: 408 phrases; correct: 223.\n",
            "\n",
            "precision:  54.66%; recall:  36.92%; F1:  44.07\n",
            "\n",
            "\t     company: precision:   60.34%; recall:   41.67%; F1:   49.30; predicted:    58\n",
            "\n",
            "\t    facility: precision:   41.18%; recall:   29.79%; F1:   34.57; predicted:    34\n",
            "\n",
            "\t     geo-loc: precision:   73.39%; recall:   55.15%; F1:   62.98; predicted:   124\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
            "\n",
            "\t musicartist: precision:   20.00%; recall:    3.70%; F1:    6.25; predicted:     5\n",
            "\n",
            "\t       other: precision:   36.05%; recall:   30.10%; F1:   32.80; predicted:    86\n",
            "\n",
            "\t      person: precision:   56.94%; recall:   39.42%; F1:   46.59; predicted:    72\n",
            "\n",
            "\t     product: precision:   18.18%; recall:    7.14%; F1:   10.26; predicted:    11\n",
            "\n",
            "\t  sportsteam: precision:   47.06%; recall:   25.81%; F1:   33.33; predicted:    17\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}